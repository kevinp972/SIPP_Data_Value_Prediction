---
title: "Predicting Total Wealth: A Predictive Analysis Using the 1991 SIPP Data"
author: "Xueshan (Kevin) Peng"
output: pdf_document
---

```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang = "en_US")
```

# Introduction
## Loading and Inspecting the Data

Let's take a look at the first 6 rows of the data.

```{r}
data <- read.table('data_tr.txt', head = T)[,-1]
head(data)
```

The variables in this dataset is defined as follows:

* tw: Total wealth (in US $), which is defined as “net financial assets, including Individual Retirement Account (IRA) and 401(k) assets, plus housing equity plus the value of business, property, and motor vehicles.” 
* ira: individual retirement account (IRA) balance (in US $). 
* e401: Binary variable, where 1 indicates eligibility for a 401(k)-retirement plan, and 0 indicates otherwise. 
* nifa: Non-401k financial assets (in US $). 
* inc: Income (in US $). 
* hmort: Home mortgage (in US $). 
* hval: Home value (in US $). 
* hequity: Home value minus home mortgage. 
* educ: Education (in years). 
* male: Binary variable, where 1 indicates male and 0 indicates otherwise. 
* twoearn: Binary variable, where 1 indicates two earners in the household, and 0 indicates otherwise. 
* nohs, hs, smcol, col: Dummy variables for education levels - no high school, high school, some college, college. 
* age: Age. 
* fsize: Family size. 
* marr: Binary variable, where 1 indicates married and 0 indicates otherwise.

```{r}
colSums(is.na(data))
any(duplicated(data))
```

We can see that the data is in good shape, where categorical variables are already transformed into dummy variables. We can also see that there exists multi-collinearity in education levels (**nohs**, **hs**, **smcol**, **col**) and home-ownership-related variables (**hmort**, **hval**, and **hequity**).

```{r}
summary(data)
```

While there exist observations where total wealth is negative, it should be noted that the variable includes home equity, which can be negative, so it does not necessarily indicate that there are incorrect data entries. 

The variables **ira**, **nohs**, **smcol**, **col**, and **male** exhibited a value of 0 at the 3rd quantile. They are probably a significant number of data points taking 
on the value of 0. Since **male** is on the list, it should also be noted that most observations are associated with female participants. 

Also, the variable **tw**, **nifa**, **hmort**, and **hequity** have means that are much greater than medians, showing signs of large outliers.

In the histogram below, we can visualize the existence of outliers with enormous wealth.

```{r}
hist(data$tw)
```

Using the graph, we can determine that removing the outliers with **tw** above $1,000,000 would be appropriate. 

```{r}
data = subset(data, data$tw<1000000)
```

## Testing and Removing Multi-collinearity

Let's test whether removing different educational level predictors affect my model's performance, gauged by (MSPE). For simplicity sake, I did not use k-fold cross validation.

```{r}
k <- 10
set.seed(123)
rand <- sample(nrow(data), floor(nrow(data)/k))
train <- setdiff(c(1:nrow(data)), rand)
y_rand <- data$tw[rand]

regnohs <- lm(tw ~ 1 + hs + smcol + col, data = data[train,])
reghs <- lm(tw ~ 1 + nohs + smcol + col, data = data[train,])
regsmcol <- lm(tw ~ 1 + nohs + hs + col, data = data[train,])
regcol <- lm(tw ~ 1 + nohs + hs + smcol, data = data[train,])

prnohs <- predict(regnohs, newdata = data[rand,])
prhs <- predict(reghs, newdata = data[rand,])
prsmcol <- predict(regsmcol, newdata = data[rand,])
prcol <- predict(regcol, newdata = data[rand,])

MSEnohs <- mean((y_rand-prnohs)^2)
MSEhs <- mean((y_rand-prhs)^2)
MSEsmcol <- mean((y_rand-prsmcol)^2)
MSEcol <- mean((y_rand-prcol)^2)

c(MSEnohs, MSEhs, MSEsmcol, MSEcol)
```

No difference in performance is found between removing different terms for multi-collinearity. For interpretability, we choose to remove **hs** for education level. 

## More Feature Selections

Since **hequity** represents home value minus home mortgage, it is intuitively a better predictor of total wealth than **hval** or **hmort** itself. Hence, choosing **hequity** over **hval** and **hmort** is the more sensible choice.

Including years of education (**educ**) along with education levels is redundant. Considering that diplomas are usually much more important than years of education, prioritizing education level over years of education is appropriate. 

```{r}
data <- data[, !(names(data) %in% c("hs", "hval", "hmort", "educ"))]
```

# Creating a Linear Baseline Model

## Using Lasso and Forward/Backward Stepwise Selection

We will strive to create a linear baseline model. This will serve as a baseline to compare to when we later add nonlinear transformations and interaction terms.

For this approach, we are going to include all the features in the dataset. We will let the feature selection algorithms, Lasso and Stepwise Selection, to select the features for us.

For better accuracy, I employed 10-fold cross validation. Leave-one-out cross validation would yield a even more accurate result, but doing so on a dataset containing 7919 observations would take too much computational power. 

```{r, warning=FALSE, message=FALSE}
source("KfoldCVFunctions.R")

mean_mspe_lasso <- compute_lasso_mspe(data, response_var = "tw")
mean_mspe_forward <- compute_forward_stepwise_mspe(data, response_var = "tw")
mean_mspe_backward <- compute_backward_stepwise_mspe(data, response_var = "tw")

# Print results
print(c(Lasso = mean_mspe_lasso, Forward_Stepwise = mean_mspe_forward, Backward_Stepwise = mean_mspe_backward))
```

As shown in the results above, Lasso yielded a lower MSPE than forward or backward stepwise selection. 

Let's now inspect the coefficients that Lasso chose and their associated p-values. 

Since Lasso performs both variable selection and shrinkage, leading to biased coefficient estimates, traditional significance tests for coefficients (like p-values) are not straightforwardly available. Therefore, we use the *hdi* (High Dimensional Inference) package to approximate the p-values.

```{r, warning=FALSE, message=FALSE}
library(hdi)

response_var <- "tw"
y <- data[[response_var]]
X <- as.matrix(data[ , !(names(data) %in% response_var)])

lasso_cv <- cv.glmnet(X, y, alpha = 1)
best_lambda <- lasso_cv$lambda.min

lasso_model <- glmnet(X, y, lambda = best_lambda, alpha = 1)

lasso_inference <- hdi::lasso.proj(X, y)

print(coef(lasso_model))
```

As shown above, Lasso has selected all of the features except nohs, col, and fsize. The coefficients are shown above.

```{r}
print(lasso_inference$pval)
```

Above are the approximated p-values the coefficients. We can gauge how strong of a predictor each feature is. As expected, ira and e401 have a really small p-value as they are literally a part of **tw**.

* tw: Total wealth (in US $), which is defined as “net financial assets, including Individual Retirement Account (IRA) and 401(k) assets, plus housing equity plus the value of business, property, and motor vehicles.” 

Let's also compute the MSPE for a simple OLS regression model and for a ridge regression model with the selected features for comparison.

```{r}
data_subset <- data[, !(names(data) %in% c("nohs", "col", "fsize"))]

source("KfoldCVFunctions.R")
mean_mspe_ols <- compute_ols_mspe(data_subset, response_var = "tw")
mean_mspe_ridge <- compute_ridge_mspe(data, response_var = "tw")

print(paste("Mean MSPE for OLS Regression:", round(mean_mspe_ols,0)))
print(paste("Mean MSPE for Ridge Regression:", round(mean_mspe_ridge,0)))
print(paste("Mean MSPE for Lasso Regression:", round(mean_mspe_lasso,0)))
print(paste("Mean MSPE for Forward Stepwise Selection:", round(mean_mspe_forward,0)))
print(paste("Mean MSPE for Backward Stepwise Selection:", round(mean_mspe_backward,0)))
```

Surprisingly, a simple OLS regression on selected features yielded better results than ridge regression, Lasso, forward/backward stepwise selection. 

The mean MSPE of our OLS regression will than serve as our benchmark, which we will strive to improve upon when fitting nonlinear transformations.

# Finding Nonlinear Relationships and Applying Transformations

## Inspecting the Relationships between **tw** and Features

After creating an appropriate linear model, the appropriate next step to improve predictive performance is through applying nonlinear transformations. 

Let's first inspect the relationships between **tw** and all the features in the dataset.

```{r}
# Load necessary libraries
library(tidyverse)
library(gridExtra)  # Alternatively, library(patchwork)

# Function to create scatterplots for each feature
plot_scatterplots <- function(data, response_var) {
  # Get feature names excluding the response variable
  feature_names <- setdiff(names(data), response_var)
  
  # Create a list to store ggplot objects
  plot_list <- list()
  
  for (feature in feature_names) {
    p <- ggplot(data = data) +
      geom_point(mapping = aes_string(x = feature, y = response_var), alpha = 0.5) +
      labs(x = feature, y = response_var) +
      theme_minimal()
    plot_list[[feature]] <- p
  }
  
  return(plot_list)
}

# Example usage
# data <- your_data_frame_here
plots <- plot_scatterplots(data_subset, response_var = "tw")

# Display plots using gridExtra
do.call(grid.arrange, c(plots, ncol = 2))  # Adjust ncol to control the number of columns

# Alternatively, display plots using patchwork
# library(patchwork)
# wrap_plots(plots, ncol = 2)  # Adjust ncol to control the number of columns

```