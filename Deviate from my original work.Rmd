---
title: "Predicting Total Wealth: A Predictive Analysis Using the 1991 SIPP Data"
author: "Xueshan (Kevin) Peng"
output: pdf_document
---

```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang = "en_US")
setwd("C:/Users/pengx/OneDrive/Documents/School/UCSD/Third Year Summer/Econ 178")
```

# Introduction
## Loading and Inspecting the Data

Let's take a look at the first 6 rows of the data.

```{r}
data <- read.table('data_tr.txt', head = T)[,-1]
head(data)
```

We can see that the data is in good shape, where categorical variables are already transformed into dummy variables. We can also see that there exists multi-collinearity in education levels (**nohs**, **hs**, **smcol**, **col**) and home-ownership-related variables (**hmort**, **hval**, and **hequity**).

```{r}
summary(data)
```

While there exist observations where total wealth is negative, it should be noted that the variable includes home mortgage, so it does not necessarily indicate that there are incorrect data entries. 

The variables **ira**, **nohs**, **smcol**, **col**, and **male** exhibited a value of 0 at the 3rd quantile. They are probably a significant number of data points taking 
on the value of 0. Since **male** is on the list, it should also be noted that most observations are associated with female participants. 

Also, the variable **tw**, **nifa**, **hmort**, and **hequity** have means that are much greater than medians, showing signs of large outliers.

In the histogram below, we can visualize the existence of outliers with enormous wealth.

```{r}
hist(data$tw)
```

Using the graph, we can determine that removing the outliers with **tw** above $1,000,000 would be appropriate. 

```{r}
data = subset(data, data$tw<1000000)
summary(data)
```

## Testing and Removing Multi-collinearity

Let's test whether removing different educational level predictors affect my model's performance, gauged by (MSPE). For simplicity sake, I did not use k-fold cross validation.

```{r}
k <- 10
set.seed(123)
rand <- sample(nrow(data), floor(nrow(data)/k))
train <- setdiff(c(1:nrow(data)), rand)
y_rand <- data$tw[rand]

regnohs <- lm(tw ~ 1 + hs + smcol + col, data = data[train,])
reghs <- lm(tw ~ 1 + nohs + smcol + col, data = data[train,])
regsmcol <- lm(tw ~ 1 + nohs + hs + col, data = data[train,])
regcol <- lm(tw ~ 1 + nohs + hs + smcol, data = data[train,])

prnohs <- predict(regnohs, newdata = data[rand,])
prhs <- predict(reghs, newdata = data[rand,])
prsmcol <- predict(regsmcol, newdata = data[rand,])
prcol <- predict(regcol, newdata = data[rand,])

MSEnohs <- mean((y_rand-prnohs)^2)
MSEhs <- mean((y_rand-prhs)^2)
MSEsmcol <- mean((y_rand-prsmcol)^2)
MSEcol <- mean((y_rand-prcol)^2)

c(MSEnohs, MSEhs, MSEsmcol, MSEcol)
```

No difference in performance is found between removing different terms for multi-collinearity. For interpretability, we choose to remove **hs** for education level. 

## More Data Cleaning

Since **hequity** represents home value minus home mortgage, it is intuitively a better predictor of total wealth than **hval** or **hmort** itself. Hence, choosing **hequity** over **hval** and **hmort** is the more sensible choice.

Including years of education (**educ**) along with education levels is redundant. Considering that diplomas are usually much more important than years of education, prioritizing education level over years of education is appropriate. 

```{r}
data <- data[, !(names(data) %in% c("hs", "hval", "hmort", "educ"))]
```

