---
title: "Predicting Total Wealth: A Predictive Analysis Using the 1991 SIPP Data"
author: "Xueshan (Kevin) Peng"
output: pdf_document
---

```{r setup, include=FALSE, tidy=TRUE}
knitr::opts_chunk$set(echo = TRUE)
Sys.setenv(lang = "en_US")
```

# Introduction
## Loading and Inspecting the Data

Let's take a look at the first 6 rows of the data.

```{r}
data <- read.table('data_tr.txt', head = T)[,-1]
head(data)
```

The variables in this dataset is defined as follows:

* tw: Total wealth (in US $), which is defined as “net financial assets, including Individual Retirement Account (IRA) and 401(k) assets, plus housing equity plus the value of business, property, and motor vehicles.” 
* ira: individual retirement account (IRA) balance (in US $). 
* e401: Binary variable, where 1 indicates eligibility for a 401(k)-retirement plan, and 0 indicates otherwise. 
* nifa: Non-401k financial assets (in US $). 
* inc: Income (in US $). 
* hmort: Home mortgage (in US $). 
* hval: Home value (in US $). 
* hequity: Home value minus home mortgage. 
* educ: Education (in years). 
* male: Binary variable, where 1 indicates male and 0 indicates otherwise. 
* twoearn: Binary variable, where 1 indicates two earners in the household, and 0 indicates otherwise. 
* nohs, hs, smcol, col: Dummy variables for education levels - no high school, high school, some college, college. 
* age: Age. 
* fsize: Family size. 
* marr: Binary variable, where 1 indicates married and 0 indicates otherwise.

```{r}
colSums(is.na(data))
any(duplicated(data))
```

We can see that the data is in good shape, where categorical variables are already transformed into dummy variables. We can also see that there exists multi-collinearity in education levels (**nohs**, **hs**, **smcol**, **col**) and home-ownership-related variables (**hmort**, **hval**, and **hequity**).

```{r}
summary(data)
```

While there exist observations where total wealth is negative, it should be noted that the variable includes home equity, which can be negative, so it does not necessarily indicate that there are incorrect data entries. 

The variables **ira**, **nohs**, **smcol**, **col**, and **male** exhibited a value of 0 at the 3rd quantile. They are probably a significant number of data points taking 
on the value of 0. Since **male** is on the list, it should also be noted that most observations are associated with female participants. 

Also, the variable **tw**, **nifa**, **hmort**, and **hequity** have means that are much greater than medians, showing signs of large outliers.

In the histogram below, we can visualize the existence of outliers with enormous wealth.

```{r}
hist(data$tw)
```

Using the graph, we can determine that removing the outliers with **tw** above $1,000,000 would be appropriate. 

```{r}
data = subset(data, data$tw<1000000)
```

## Testing and Removing Multi-collinearity

Let's test whether removing different educational level predictors affect my model's performance, gauged by (MSPE). For simplicity sake, I did not use k-fold cross validation.

```{r}
k <- 10
set.seed(123)
rand <- sample(nrow(data), floor(nrow(data)/k))
train <- setdiff(c(1:nrow(data)), rand)
y_rand <- data$tw[rand]

regnohs <- lm(tw ~ 1 + hs + smcol + col, data = data[train,])
reghs <- lm(tw ~ 1 + nohs + smcol + col, data = data[train,])
regsmcol <- lm(tw ~ 1 + nohs + hs + col, data = data[train,])
regcol <- lm(tw ~ 1 + nohs + hs + smcol, data = data[train,])

prnohs <- predict(regnohs, newdata = data[rand,])
prhs <- predict(reghs, newdata = data[rand,])
prsmcol <- predict(regsmcol, newdata = data[rand,])
prcol <- predict(regcol, newdata = data[rand,])

MSEnohs <- mean((y_rand-prnohs)^2)
MSEhs <- mean((y_rand-prhs)^2)
MSEsmcol <- mean((y_rand-prsmcol)^2)
MSEcol <- mean((y_rand-prcol)^2)

c(MSEnohs, MSEhs, MSEsmcol, MSEcol)
```

No difference in performance is found between removing different terms for multi-collinearity. For interpretability, we choose to remove **hs** for education level. 

## More Feature Selections

Since **hequity** represents home value minus home mortgage, it is intuitively a better predictor of total wealth than **hval** or **hmort** itself. Hence, choosing **hequity** over **hval** and **hmort** is the more sensible choice.

Including years of education (**educ**) along with education levels is redundant. Considering that diplomas are usually much more important than years of education, prioritizing education level over years of education is appropriate. 

```{r}
data <- data[, !(names(data) %in% c("hs", "hval", "hmort", "educ"))]
```

# Creating a Linear Baseline Model

## Using Lasso and Forward/Backward Stepwise Selection

We will strive to create a linear baseline model. This will serve as a baseline to compare to when we later add nonlinear transformations and interaction terms.

For this approach, we are going to include all the features in the dataset. We will let the feature selection algorithms, Lasso and Stepwise Selection, to select the features for us.

For better accuracy, I employed 10-fold cross validation. Leave-one-out cross validation would yield a even more accurate result, but doing so on a dataset containing 7919 observations would take too much computational power. 

```{r, warning=FALSE, message=FALSE}
source("KfoldCVFunctions.R")

mean_mspe_lasso <- compute_lasso_mspe(data, response_var = "tw")
mean_mspe_forward <- compute_forward_stepwise_mspe(data, response_var = "tw")
mean_mspe_backward <- compute_backward_stepwise_mspe(data, response_var = "tw")

# Print results
print(c(Lasso = mean_mspe_lasso, 
        Forward_Stepwise = mean_mspe_forward, 
        Backward_Stepwise = mean_mspe_backward))
```

As shown in the results above, Lasso yielded a lower MSPE than forward or backward stepwise selection. 

Let's now inspect the coefficients that Lasso chose and their associated p-values. 

Since Lasso performs both variable selection and shrinkage, leading to biased coefficient estimates, traditional significance tests for coefficients (like p-values) are not straightforwardly available. Therefore, we use the *hdi* (High Dimensional Inference) package to approximate the p-values.

```{r, warning=FALSE, message=FALSE}
library(hdi)

response_var <- "tw"
y <- data[[response_var]]
X <- as.matrix(data[ , !(names(data) %in% response_var)])

lasso_cv <- cv.glmnet(X, y, alpha = 1)
best_lambda <- lasso_cv$lambda.min

lasso_model <- glmnet(X, y, lambda = best_lambda, alpha = 1)

lasso_inference <- hdi::lasso.proj(X, y)

print(coef(lasso_model))
```

As shown above, Lasso has selected all of the features except nohs, col, and fsize. The coefficients are shown above.

```{r}
print(lasso_inference$pval)
```

Above are the approximated p-values the coefficients. We can gauge how strong of a predictor each feature is. As expected, ira and e401 have a really small p-value as they are literally a part of **tw**.

* tw: Total wealth (in US $), which is defined as “net financial assets, including Individual Retirement Account (IRA) and 401(k) assets, plus housing equity plus the value of business, property, and motor vehicles.” 

Let's also compute the MSPE for a simple OLS regression model and for a ridge regression model with the selected features for comparison.

```{r}
data_subset <- data[, !(names(data) %in% c("nohs", "col", "fsize"))]

source("KfoldCVFunctions.R")
mean_mspe_ols <- compute_ols_mspe(data_subset, response_var = "tw")
mean_mspe_ridge <- compute_ridge_mspe(data_subset, response_var = "tw")

print(c(
  OLS = mean_mspe_ols, 
  Ridge = mean_mspe_ridge, 
  Lasso = mean_mspe_lasso, 
  Forward_Stepwise = mean_mspe_forward, 
  Backward_Stepwise = mean_mspe_backward
))
```

Surprisingly, a simple OLS regression on selected features yielded better results than ridge regression, Lasso, forward/backward stepwise selection. 

The mean MSPE of our OLS regression will than serve as our benchmark, which we will strive to improve upon when fitting nonlinear transformations.

```{r}
benchmark = mean_mspe_ols
```

# Finding Nonlinear Relationships and Applying Transformations

## Inspecting the Relationships between **tw** and Features

After creating an appropriate linear model, the appropriate next step to improve predictive performance is through applying nonlinear transformations. 

Let's first inspect the relationships between **tw** and all the quantitative (non-binary) features in the dataset. Nonlinear transformations are inappropriate for binary features because they only have two distinct values, making such transformations ineffective and potentially meaningless.

The below function compiles all the scatterplots into a list of *ggplot* objects.

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
library(gridExtra) 

plot_scatterplots <- function(data, response_var, feature_names) {
  
  plot_list <- list()
  
  for (feature in feature_names) {
    p <- ggplot(data = data) +
      geom_point(mapping = aes(x = !!sym(feature), y = !!sym(response_var)), 
                 alpha = 0.5) +
      labs(x = feature, y = response_var) +
      theme_minimal() +
      theme(aspect.ratio = 1)
    plot_list[[feature]] <- p
  }
  
  return(plot_list)
}
```

We can then call the function and use the *gridExtra* package to arrange them.

```{r}
plots <- plot_scatterplots(data_subset, 
                           response_var = "tw", 
                           c("ira", "nifa", "inc", "hequity", "age"))

do.call(grid.arrange, c(plots, ncol = 3)) 
```

Since **tw** is defined as net financial assets including **ira**, it is expected that there is a linear relationship between **tw** and **ira**, and this is what we can see on the scatterplot, too. The same goes with **hequity**.

Similarly, **nifa** is defined as non-401k financial assets and has a seemingly linear relationship with **tw** despite it having a more data points on the lower end than the higher end of **nifa**.

One that stands out as being nonlinear is the relationship between **age** and **tw**. In the area where there are more data points, represented by a darker color, there seems to be a sharper increase in **tw** at a younger age. 

## Applying Nonlinear Transformations to Age

For the following reasons, I will be using cubic splines as opposed to polynomials.

* Polynomials must use a high degree for flexible fits, but splines are able to do so with the degree fixed. This is likely to produce more stable estimates.
* Polynomials lack the ability to incorporate thresholds like splines, leading to undesirably global outcomes. In other words, observations within one range of the predictor strongly influence the model's behavior across different ranges.
* A polynomial fit is likely to produce undesirable results at the boundaries.

I have constructed a function to find the optimal number of knots and also where to place them. The function returns a set of knots, boundary knots (endpoints of the feature), as well as a plot showing the MSE vs. Number of Knots.

```{r}
source("select_optimal_knots.R")

result = select_optimal_knots(data, "tw", "age", 20)
print(result$knots_chosen)
```

We can see that the algorithms returns the optimal set of knots, which, in this case, contains 10 items.

```{r}
print(result$boundary_knots)
print(result$plot)
```

We can see through the visualization that the MSE is the lowest when the number of knots is 10.

```{r}
age_spline_model <- lm(tw ~ bs(age, 
                               knots = result$knots_chosen, 
                               Boundary.knots = result$boundary_knots), 
                       data = data)

x.grid <- seq(from = result$boundary_knots[1], 
              to = result$boundary_knots[2], 
              length.out = 1000)
pr <- predict(age_spline_model, newdata = data.frame(age = x.grid))

predictions <- data.frame(age = x.grid, tw = pr)

ggplot(data = data) + 
  geom_point(mapping = aes(x = age, y = tw)) +
  geom_line(data = predictions, 
            mapping = aes(x = age, y = tw), 
            lwd = 1, 
            color = "blue")
```

We can then use the previous output to construct a cubic spline model, from which predictions can be obtained and visualized through the graph.

However, the visualization is suggesting that there might be some overfitting of the data, as there is no reason for wealth to have a jump at the age of 62. We will validate this through the below analysis.

```{r}
source("spline_transform_dataset.R")

transformed_data <- spline_transform_dataset(data, 
                                             "age", 
                                             result$knots_chosen, 
                                             result$boundary_knots)
transformed_data_sub <- spline_transform_dataset(data_subset, 
                                                 "age", 
                                                 result$knots_chosen, 
                                                 result$boundary_knots)

source("KfoldCVFunctions.R")

mean_mspe_lasso <- compute_lasso_mspe(transformed_data, response_var = "tw")
mean_mspe_forward <- compute_forward_stepwise_mspe(transformed_data, response_var = "tw")
mean_mspe_backward <- compute_backward_stepwise_mspe(transformed_data, response_var = "tw")
mean_mspe_ols <- compute_ols_mspe(transformed_data_sub, response_var = "tw")
mean_mspe_ridge <- compute_ridge_mspe(transformed_data_sub, response_var = "tw")

print(c(
  OLS_without_transformations = benchmark,
  OLS = mean_mspe_ols, 
  Ridge = mean_mspe_ridge, 
  Lasso = mean_mspe_lasso, 
  Forward_Stepwise = mean_mspe_forward, 
  Backward_Stepwise = mean_mspe_backward
))
```

Applying the transformation to **age** and incorporating it into the dataset, we can now use General Additive Method (GAM) to compare the model's performance with the benchmark.

Confirming our previous guess about overfitting, the original linear model yielded a better performance, suggesting that the relationship between **tw** and **age** is approximately linear.

## Applying Nonlinear Transformations to Income (inc)

Let's now inspect the relationship between **tw** and **inc**, which looks a giant lump. This might suggest a nonlinear relationship.

```{r}
ggplot(data = data) + 
  geom_point(mapping = aes(x = inc, y = tw))
```

Notice that there are less observations on the higher end of the variable **inc**. Hence, a natural cubic spline would help reduce the variance which is otherwise substantial when **inc** takes on a large value. In the region where X is smaller than the smallest knot, or 
larger than the largest knot, a natural cubic spline constrains the fit to be linear. 

```{r}
source("select_optimal_knots_natural.R")

result = select_optimal_knots_natural(data, "tw", "inc", 8)
print(result$plot)
```

The optimal number of knots is 3.

```{r}
inc_spline_model <- lm(tw ~ ns(inc, 
                               knots = result$knots_chosen, 
                               Boundary.knots = result$boundary_knots), 
                       data = data)

x.grid <- seq(from = result$boundary_knots[1], 
              to = result$boundary_knots[2], 
              length.out = 1000)
pr <- predict(inc_spline_model, newdata = data.frame(inc = x.grid))

predictions <- data.frame(inc = x.grid, tw = pr)

ggplot(data = data) + 
  geom_point(mapping = aes(x = inc, y = tw)) +
  geom_line(data = predictions, 
            mapping = aes(x = inc, y = tw), 
            lwd = 1, 
            color = "blue")
```

The line fitted by the natural spline model is shown above.

```{r}
source("natural_spline_transform_dataset.R")

transformed_data <- natural_spline_transform_dataset(data, 
                                                     "inc", 
                                                     result$knots_chosen, 
                                                     result$boundary_knots)
transformed_data_sub <- natural_spline_transform_dataset(data_subset, 
                                                         "inc", 
                                                         result$knots_chosen, 
                                                         result$boundary_knots)

source("KfoldCVFunctions.R")

mean_mspe_lasso <- compute_lasso_mspe(transformed_data, response_var = "tw")
mean_mspe_forward <- compute_forward_stepwise_mspe(transformed_data, response_var = "tw")
mean_mspe_backward <- compute_backward_stepwise_mspe(transformed_data, response_var = "tw")
mean_mspe_ols <- compute_ols_mspe(transformed_data_sub, response_var = "tw")
mean_mspe_ridge <- compute_ridge_mspe(transformed_data_sub, response_var = "tw")

print(c(
  OLS_without_transformations = benchmark,
  OLS = mean_mspe_ols, 
  Ridge = mean_mspe_ridge, 
  Lasso = mean_mspe_lasso, 
  Forward_Stepwise = mean_mspe_forward, 
  Backward_Stepwise = mean_mspe_backward
))
```

Once again, the original linear model yielded a better performance, suggesting that the relationship between **tw** and **inc** is approximately linear.

## Conclusion for Finding Nonlinearity 

Using the definition of **tw**, we have ascertained the linearity of its relationship with each of the following:

* **ira**
* **nifa**
* **hequity**

Also, applying General Additive Method and spline basis representation did not yield a better predictive performance, implying that the relationship of **tw** with **age** and **inc** are approximately linear as well.

We can therefore conclude that we have encountered bad luck with this dataset, where there are almost no room for nonlinear models to help improve predictive power.

# Adding Interaction Terms

Adding interaction terms to a model can improve its predictive performance, especially if there are meaningful interactions between variables.

As mentioned before, based on the definition of **tw**, **tw** contains **ira**, **nifa**, and **hequity**. Hence, the coefficient to these variables should be close to 1 and shall not be influenced by other variables. Hence we will not consider these three variables for interaction terms.

Using a custom-built function, we can input the variable we are predicting and the variables we want to exclude to obtain a dataset with all possible interaction terms.

```{r}
source("create_interactions.R")

data_with_interactions <- create_interactions(data_subset, "tw", c("ira", "nifa", "hequity"))

print(colnames(data_with_interactions))
```

Let's check whether we have the right columns. It seems that the function has done its job, obtaining 11 + (6 + 5 + 4 + 3 + 2 + 1) = 32 columns.

```{r}
source("KfoldCVFunctions.R")

mean_mspe_lasso <- compute_lasso_mspe(data_with_interactions, response_var = "tw")
mean_mspe_forward <- compute_forward_stepwise_mspe(data_with_interactions, response_var = "tw")
mean_mspe_backward <- compute_backward_stepwise_mspe(data_with_interactions, response_var = "tw")

print(c(
  OLS_without_interactions = benchmark,
  Lasso = mean_mspe_lasso, 
  Forward_Stepwise = mean_mspe_forward, 
  Backward_Stepwise = mean_mspe_backward
))
```

Based on the output above, we can conclude that adding interaction terms is the right choice. Using all three feature selection methods, we are able to obtain a better predictive performance than the benchmark.

Backward stepwise selection has yielded the best performance. Let's check what interaction terms it has chosen for our model.

```{r}
full <- lm(as.formula(paste(response_var, "~ .")), data=data_with_interactions)
null <- lm(as.formula(paste(response_var, "~ 1")), data=data_with_interactions)

backward <- stepAIC(full, scope=list(lower=null, upper=full), trace = FALSE, direction='backward')

summary(backward)
```

It is looking promising as most of the features are highly significant with very small p-values.

The choice of interaction terms is intuitive, too. Just as an example, if a person is eligible for 401k retirement plan, which means the value of **e401** is 1, he/she is likely to accumulate more wealth with age than a person who is not eligible for the retirement plan. Hence, the coefficient of the interaction term **e401:age** would naturally be positive.

Let's visualize this on a graph.

```{r}
ggplot(data, aes(x = age, y = tw, color = factor(e401))) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  labs(title = "Interaction between E401 and Age on TW")
```

This is just one of the examples. All others interactions make intuitive sense if you think hard enough. I am going to skip the explanations for the purpose of not making this Markdown file too long.

Let's now try the selected features with OLS and Ridge Regression.

```{r}
data_with_selected_interactions <- data_subset %>%
    mutate(
      `e401:inc` = e401 * inc,
      `e401:age` = e401 * age,
      `inc:male` = inc * male,
      `inc:twoearn` = inc * twoearn,
      `inc:age` = inc * age,
      `inc:marr` = inc * marr,
      `male:smcol` = male * smcol,
      `twoearn:age` = twoearn * age
    ) %>%
    select(-smcol)

mean_mspe_ols <- compute_ols_mspe(data_with_selected_interactions, response_var = "tw")
mean_mspe_ridge <- compute_ridge_mspe(data_with_selected_interactions, response_var = "tw")

print(c(
  OLS_without_interactions = benchmark,
  OLS = mean_mspe_ols, 
  Ridge = mean_mspe_ridge, 
  Lasso = mean_mspe_lasso, 
  Forward_Stepwise = mean_mspe_forward, 
  Backward_Stepwise = mean_mspe_backward
))
```

With the selected features, our OLS model is once again able to lower the MSPE obtained through the 10-fold cross validation process.

This will conclude our selection for interaction terms. Let's now construct the final model.

# Conclusion

## Constructing the Final Model

```{r}
formula <- as.formula(paste(response_var, "~ ."))
final_model <- lm(formula, data=data_with_selected_interactions)
summary(final_model)
```

Here we have the final model. It is unfortunate that we are unable to include any transformations for nonlinearity without impacting the predictive performance, but doing so is more suitable accounting for the linearity of the relationship between **tw** and features.

The coefficients and the associated p-values are shown above.

```{r}
plot(final_model, which = 1)
```

The model demonstrates a strong fit to the data, as evidenced by the mostly flat residuals versus fitted values graph. This indicates that the model performs well across all areas of the data.

```{r}
plot(final_model, which = 5)
```

Our model effectively identifies and excludes the correct outliers, as all data points fall well within a Cook's distance of 0.5. This indicates that no single point is unduly influencing the model's results.

## Loading the Testing Data for Predictions

With the model constructed, we can now create predictions.

```{r}
data_te <- read.table("data_for_prediction.txt", header = TRUE, sep = "\t", dec = ".")[,-1]

# data_te <- data_te[, !(names(data) %in% c("hs", "hval", "hmort", "educ", "nohs", "col", "fsize", "smcol"))]

data_te <- data_te %>%
    mutate(
      `e401:inc` = e401 * inc,
      `e401:age` = e401 * age,
      `inc:male` = inc * male,
      `inc:twoearn` = inc * twoearn,
      `inc:age` = inc * age,
      `inc:marr` = inc * marr,
      `male:smcol` = male * smcol,
      `twoearn:age` = twoearn * age
    )

predictions <- predict(final_model, newdata=data_te)
data_te <- cbind(predicted_tw = predictions, data_te)
```


